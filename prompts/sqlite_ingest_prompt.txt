You are a senior data engineer. Using the expanded synthetic e-commerce CSV files (customers.csv, products.csv, orders.csv, order_items.csv, inventory_events.csv, marketing_campaigns.csv, support_tickets.csv) in the ./data directory, produce a Python 3 script named load_ecommerce_data.py that loads the data into a SQLite database called ecommerce.db.

Requirements:
- Use only built-in libraries plus sqlite3, csv, argparse, pathlib, and typing.
- Create normalized tables: customers, products, orders, order_items, inventory_events, marketing_campaigns, support_tickets, and an associative table order_campaign_fact (order_id + campaign_id).
- Explicitly define CREATE TABLE statements with appropriate column types (TEXT, INTEGER, REAL, NUMERIC) and primary/foreign keys matching the CSV schemas. Include NOT NULL constraints where reasonable.
- Add CHECK constraints for enumerations (e.g., loyalty_tier, resolution_status) and ensure total_amount ≈ subtotal + shipping_cost + tax_amount - discount.
- Wrap inserts in transactions and use executemany for efficiency while streaming structured logs (timestamp + level).
- Coerce CSV values into correct types (e.g., decimals as float, timestamps as text, booleans to integers) via centralized helper functions.
- Enforce referential integrity with FOREIGN KEY constraints (ensure PRAGMA foreign_keys = ON).
- Handle idempotency: drop existing tables before recreating, or delete existing rows and vacuum.
- Provide CLI arguments: --data-dir, --database, --drop-tables, --vacuum, --dry-run (schema + CSV validation only).
- Emit data quality stats per table (row counts, min/max timestamps, orphaned keys) and summarize at the end.
- Include docstrings, type hints, and `if __name__ == "__main__": main()` entrypoint.
- Append a README-style triple-quoted string detailing execution steps, validation logic, and troubleshooting tips.

Return the full load_ecommerce_data.py file content only; no extra analysis.
